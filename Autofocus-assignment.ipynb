{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "editable": false,
    "deletable": false
   },
   "source": [
    "# <font style = \"color:rgb(50,120,229)\">Auto Focus Assignment</font>\n",
    "\n",
    "Autofocus is implemented in all digital cameras these days. \n",
    "\n",
    "While using your phone camera, you may have noticed, the camera goes out of focus for a second or two, and the image looks blurry for a bit. The camera quickly performs some calculations and autofocuses to bring the subject in focus. \n",
    "\n",
    "In SLR cameras, autofocus is activated when we press the button half way through. You can see and hear parts of the lens moving as the camera tries to autofocus. \n",
    "\n",
    "Whether it is an SLR camera or your phone camera, autofocussing is done by taking a series of photos of the scene while changing the distance of the image sensor from the lens inside the camera.\n",
    "\n",
    "In this assignment, we will find the sharpest image in a video squence of a static scene. In essence, we will do the computation necessary for autofocussing. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "editable": false,
    "deletable": false,
    "ExecuteTime": {
     "end_time": "2025-01-20T23:46:33.717813Z",
     "start_time": "2025-01-20T23:46:33.144648Z"
    }
   },
   "source": [
    "# Import modules\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from dataPath import DATA_PATH\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "editable": false,
    "deletable": false,
    "ExecuteTime": {
     "end_time": "2025-01-20T23:46:33.726037Z",
     "start_time": "2025-01-20T23:46:33.721800Z"
    }
   },
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "matplotlib.rcParams['image.interpolation'] = 'bilinear'"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "editable": false,
    "deletable": false
   },
   "source": [
    "# <font style = \"color:rgb(50,120,229)\">Measures of Sharpness</font>\n",
    "\n",
    "How do we know if an image is sharp? What is a good measure of sharpness?\n",
    "\n",
    "As you can imagine, an out of focus image is smooth and does not have large gradient. So some function of the gradient (first derivative) of an image should help you. \n",
    "\n",
    "A different measure could be based on the second order derivative of the image called the Laplacian. \n",
    "\n",
    "In this assignment, you to have to read one paper and a section of another paper to figure out the sharpest image in a video sequence. \n",
    "\n",
    "1. [Diatom autofocusing in brightheld microscopy: a comparative study](https://www.researchgate.net/profile/Jose-Luis-Pech-Pacheco/publication/3887632_Diatom_autofocusing_in_brightfield_microscopy_A_comparative_study/links/598c5303458515c333a828f5/Diatom-autofocusing-in-brightfield-microscopy-A-comparative-study.pdf)): This paper has several measures of sharpess. \n",
    "\n",
    "2. [Shape from Focus](https://graphics.stanford.edu/courses/cs348b-06/homework3/Nayar_CVPR92.pdf): This paper is about estimating the 3D shape of a scene using focus information. In Section 5, the author discusses a measure of focus. \n",
    "\n",
    "In the above papers, the focus is calculated over small windows. For our assignement, the focus measure needs to be calcualted over the entire image and not a small window. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "editable": false,
    "deletable": false
   },
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Assignment Instructions</font>\n",
    "\n",
    "In this assignment, you have to implement the following measures of focus:\n",
    "\n",
    "1. **Variance of absolute values of Laplacian** - `var_abs_laplacian` function\n",
    "\n",
    "2. **Sum Modified Laplacian (SML)** - `sum_modified_laplacian` function\n",
    "\n",
    "You will also have to manually specify the ROI for the flower region in the frame so that instead of carrying out the auto focus measurement on the entire frame, it can be carried out only on the flower.\n",
    "\n",
    "## <font style = \"color:rgb(50,120,229)\">Grading Rubric</font>\n",
    "\n",
    "The assignment carries **30 marks** and the marks distribution is as follows.\n",
    "\n",
    "1. Implementing *Variance of absolute values of Laplacian* method - **10 marks** (Autograded)\n",
    "2. Implementing *Sum Modified Laplacian (SML)* method - **10 marks** (Autograded)\n",
    "3. Overall submission - **10 marks** (Manually graded)\n",
    "\n",
    "A total of **5 submissions** will be allowed.\n",
    "\n",
    "## <font style = \"color:rgb(50,120,229)\">Important Functions</font>\n",
    "\n",
    "Here are some important functions that you can use in this assignment.\n",
    "\n",
    "1. [`cv2.filter2D`](https://docs.opencv.org/4.1.0/d4/d86/group__imgproc__filter.html#ga27c049795ce870216ddfb366086b5a04): Convolves an image with the kernel. The function applies an arbitrary linear filter to an image.\n",
    "\n",
    "2. [`cv2.Laplacian`](https://docs.opencv.org/4.1.0/d4/d86/group__imgproc__filter.html#gad78703e4c8fe703d479c1860d76429e6): Calculates the Laplacian of an image.\n",
    "\n",
    "\n",
    "#### <font style = \"color:rgb(200,0,0)\">Note</font>\n",
    "Please do not look for code online!  \n",
    "\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-20T23:46:40.502399Z",
     "start_time": "2025-01-20T23:46:40.497195Z"
    }
   },
   "source": [
    "# Implement Variance of absolute values of Laplacian - Method 1\n",
    "# Input: image\n",
    "# Output: Floating point number denoting the measure of sharpness of image\n",
    "\n",
    "# Do NOT change the function name and definition\n",
    "\n",
    "def var_abs_laplacian(image):\n",
    "    # Calculate the Laplacian of the image\n",
    "    laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
    "    \n",
    "    # Take the absolute value of the Laplacian\n",
    "    abs_laplacian = np.abs(laplacian)\n",
    "    \n",
    "    # Calculate the variance of the absolute values\n",
    "    variance = np.var(abs_laplacian)\n",
    "    \n",
    "    return variance\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-20T23:46:43.329469Z",
     "start_time": "2025-01-20T23:46:43.324129Z"
    }
   },
   "source": [
    "# Implement Sum Modified Laplacian - Method 2\n",
    "# Input: image\n",
    "# Output: Floating point number denoting the measure of sharpness of image\n",
    "\n",
    "# Do NOT change the function name and definition\n",
    "\n",
    "def sum_modified_laplacian(im):\n",
    "    # Define the kernels for the modified Laplacian\n",
    "    kernel_x = np.array([[0, 0, 0], [-1, 2, -1], [0, 0, 0]])\n",
    "    kernel_y = np.array([[0, -1, 0], [0, 2, 0], [0, -1, 0]])\n",
    "\n",
    "    # Apply the kernels to the image\n",
    "    laplacian_x = cv2.filter2D(im, cv2.CV_64F, kernel_x)\n",
    "    laplacian_y = cv2.filter2D(im, cv2.CV_64F, kernel_y)\n",
    "\n",
    "    # Calculate the sum of the absolute values of the results\n",
    "    sum_modified_laplacian = np.sum(np.abs(laplacian_x) + np.abs(laplacian_y))\n",
    "\n",
    "    return sum_modified_laplacian\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "editable": false,
    "deletable": false
   },
   "source": [
    "Let's have a look at the input video.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center>\n",
    "<video controls width=800 src=\"https://www.dropbox.com/s/p3z0hbgo7sacqd1/focus-test.mp4?dl=1\" type=\"video/mp4\" />\n",
    "</center>"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T23:47:22.028796Z",
     "start_time": "2025-01-20T23:46:49.869044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read input video filename\n",
    "filename = \"./focus-test.mp4\"\n",
    "\n",
    "# Create a VideoCapture object\n",
    "cap = cv2.VideoCapture(filename)\n",
    "\n",
    "# Read first frame from the video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "if not ret:\n",
    "    print(\"Failed to read the video file.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "# Select ROI\n",
    "\n",
    "roi = cv2.selectROI(\"Select ROI\", frame, fromCenter=False, showCrosshair=True)\n",
    "# Wait for a key press to close the window\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow(\"Select ROI\")\n",
    "\n",
    "# Extract ROI coordinates\n",
    "x, y, w, h = roi\n",
    "top, left, bottom, right = y, x, y + h, x + w\n",
    "\n",
    "print(f\"Top: {top}, Left: {left}, Bottom: {bottom}, Right: {right}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top: 46, Left: 418, Bottom: 649, Right: 1073\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-20T23:04:52.650079Z",
     "start_time": "2025-01-20T23:04:37.062167Z"
    }
   },
   "source": [
    "# Read input video filename\n",
    "filename = './focus-test.mp4'\n",
    "\n",
    "# Create a VideoCapture object\n",
    "cap = cv2.VideoCapture(filename)\n",
    "\n",
    "# Read first frame from the video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Display total number of frames in the video\n",
    "print(\"Total number of frames : {}\".format(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))))\n",
    "\n",
    "maxV1 = 0\n",
    "maxV2 = 0\n",
    "\n",
    "# Frame with maximum measure of focus\n",
    "# Obtained using methods 1 and 2\n",
    "bestFrame1 = 0 \n",
    "bestFrame2 = 0 \n",
    "\n",
    "# Frame ID of frame with maximum measure\n",
    "# of focus\n",
    "# Obtained using methods 1 and 2\n",
    "bestFrameId1 = 0 \n",
    "bestFrameId2 = 0 \n",
    "\n",
    "# Specify the ROI for flower in the frame\n",
    "# UPDATE THE VALUES BELOW\n",
    "#top = 0\n",
    "#left = 0\n",
    "#bottom = frame.shape[0]\n",
    "#right = frame.shape[1]\n",
    "\n",
    "# Iterate over all the frames present in the video\n",
    "while(ret):\n",
    "    # Crop the flower region out of the frame\n",
    "    flower = frame[top:bottom, left:right]\n",
    "    # Get measures of focus from both methods\n",
    "    val1 = var_abs_laplacian(flower)\n",
    "    val2 = sum_modified_laplacian(flower) \n",
    "    frame_id = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "    print(\"Frame: %d, VAR_LAP: %f, SML: %d\" % (frame_id,val1,val2))\n",
    "    \n",
    "    # If the current measure of focus is greater \n",
    "    # than the current maximum\n",
    "    if val1 > maxV1 :\n",
    "        # Revise the current maximum\n",
    "        maxV1 = val1\n",
    "        # Get frame ID of the new best frame\n",
    "        bestFrameId1 = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        # Revise the new best frame\n",
    "        bestFrame1 = frame.copy()\n",
    "        print(\"Frame ID of the best frame [Method 1]: {}\".format(bestFrameId1))\n",
    "\n",
    "    # If the current measure of focus is greater \n",
    "    # than the current maximum\n",
    "    if val2 > maxV2 : \n",
    "        # Revise the current maximum\n",
    "        maxV2 = val2\n",
    "        # Get frame ID of the new best frame\n",
    "        bestFrameId2 = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        # Revise the new best frame\n",
    "        bestFrame2 = frame.copy()\n",
    "        print(\"Frame ID of the best frame [Method 2]: {}\".format(bestFrameId2))\n",
    "        \n",
    "    # Read a new frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "\n",
    "print(\"================================================\")\n",
    "# Print the Frame ID of the best frame\n",
    "print(\"Frame ID of the best frame [Method 1]: {}\".format(bestFrameId1))\n",
    "print(\"Frame ID of the best frame [Method 2]: {}\".format(bestFrameId2))\n",
    "\n",
    "# Release the VideoCapture object\n",
    "cap.release()\n",
    "\n",
    "# Stack the best frames obtained using both methods\n",
    "out = np.hstack((bestFrame1, bestFrame2))\n",
    "\n",
    "# Display the stacked frames\n",
    "plt.figure()\n",
    "plt.imshow(out[:,:,::-1])\n",
    "plt.axis('off')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames : 186\n",
      "Frame: 1, VAR_LAP: 5.053682, SML: 6463981\n",
      "Frame ID of the best frame [Method 1]: 1\n",
      "Frame ID of the best frame [Method 2]: 1\n",
      "Frame: 2, VAR_LAP: 4.587558, SML: 6336910\n",
      "Frame: 3, VAR_LAP: 4.962127, SML: 6557052\n",
      "Frame ID of the best frame [Method 2]: 3\n",
      "Frame: 4, VAR_LAP: 4.074386, SML: 5922554\n",
      "Frame: 5, VAR_LAP: 4.722467, SML: 6274964\n",
      "Frame: 6, VAR_LAP: 4.447176, SML: 6254757\n",
      "Frame: 7, VAR_LAP: 4.696886, SML: 6324512\n",
      "Frame: 8, VAR_LAP: 4.518406, SML: 6307549\n",
      "Frame: 9, VAR_LAP: 5.255025, SML: 6793638\n",
      "Frame ID of the best frame [Method 1]: 9\n",
      "Frame ID of the best frame [Method 2]: 9\n",
      "Frame: 10, VAR_LAP: 4.433825, SML: 6351723\n",
      "Frame: 11, VAR_LAP: 4.664687, SML: 6479974\n",
      "Frame: 12, VAR_LAP: 4.430483, SML: 6324546\n",
      "Frame: 13, VAR_LAP: 5.203155, SML: 6841338\n",
      "Frame ID of the best frame [Method 2]: 13\n",
      "Frame: 14, VAR_LAP: 4.325425, SML: 6262201\n",
      "Frame: 15, VAR_LAP: 4.622311, SML: 6408725\n",
      "Frame: 16, VAR_LAP: 4.175331, SML: 6185681\n",
      "Frame: 17, VAR_LAP: 5.066851, SML: 6775063\n",
      "Frame: 18, VAR_LAP: 4.016370, SML: 6026521\n",
      "Frame: 19, VAR_LAP: 4.412048, SML: 6286843\n",
      "Frame: 20, VAR_LAP: 4.000726, SML: 6045728\n",
      "Frame: 21, VAR_LAP: 5.133096, SML: 6746921\n",
      "Frame: 22, VAR_LAP: 4.183471, SML: 6170791\n",
      "Frame: 23, VAR_LAP: 3.948438, SML: 6043100\n",
      "Frame: 24, VAR_LAP: 4.808236, SML: 6611290\n",
      "Frame: 25, VAR_LAP: 4.423440, SML: 6159460\n",
      "Frame: 26, VAR_LAP: 4.067006, SML: 5994043\n",
      "Frame: 27, VAR_LAP: 3.949728, SML: 5936084\n",
      "Frame: 28, VAR_LAP: 3.857303, SML: 5934890\n",
      "Frame: 29, VAR_LAP: 4.680774, SML: 6521799\n",
      "Frame: 30, VAR_LAP: 3.835117, SML: 5943998\n",
      "Frame: 31, VAR_LAP: 3.978988, SML: 6045017\n",
      "Frame: 32, VAR_LAP: 3.735563, SML: 5903877\n",
      "Frame: 33, VAR_LAP: 4.686883, SML: 6600356\n",
      "Frame: 34, VAR_LAP: 3.873206, SML: 6053043\n",
      "Frame: 35, VAR_LAP: 3.946928, SML: 6073458\n",
      "Frame: 36, VAR_LAP: 3.833506, SML: 6019413\n",
      "Frame: 37, VAR_LAP: 4.558587, SML: 6553013\n",
      "Frame: 38, VAR_LAP: 3.770898, SML: 5968522\n",
      "Frame: 39, VAR_LAP: 3.928476, SML: 6076603\n",
      "Frame: 40, VAR_LAP: 3.721932, SML: 5922765\n",
      "Frame: 41, VAR_LAP: 4.439315, SML: 6459031\n",
      "Frame: 42, VAR_LAP: 3.750010, SML: 5907462\n",
      "Frame: 43, VAR_LAP: 3.958558, SML: 6020209\n",
      "Frame: 44, VAR_LAP: 3.737587, SML: 5883671\n",
      "Frame: 45, VAR_LAP: 4.633794, SML: 6451712\n",
      "Frame: 46, VAR_LAP: 3.810592, SML: 5869099\n",
      "Frame: 47, VAR_LAP: 4.241676, SML: 6124447\n",
      "Frame: 48, VAR_LAP: 4.004960, SML: 6032019\n",
      "Frame: 49, VAR_LAP: 4.981893, SML: 6595218\n",
      "Frame: 50, VAR_LAP: 4.370297, SML: 6222207\n",
      "Frame: 51, VAR_LAP: 4.370253, SML: 6182623\n",
      "Frame: 52, VAR_LAP: 4.233506, SML: 6156914\n",
      "Frame: 53, VAR_LAP: 5.618185, SML: 6858053\n",
      "Frame ID of the best frame [Method 1]: 53\n",
      "Frame ID of the best frame [Method 2]: 53\n",
      "Frame: 54, VAR_LAP: 4.523910, SML: 6350243\n",
      "Frame: 55, VAR_LAP: 4.780824, SML: 6479295\n",
      "Frame: 56, VAR_LAP: 4.734105, SML: 6474341\n",
      "Frame: 57, VAR_LAP: 6.255153, SML: 7178307\n",
      "Frame ID of the best frame [Method 1]: 57\n",
      "Frame ID of the best frame [Method 2]: 57\n",
      "Frame: 58, VAR_LAP: 4.825012, SML: 6487712\n",
      "Frame: 59, VAR_LAP: 5.176883, SML: 6661883\n",
      "Frame: 60, VAR_LAP: 5.079031, SML: 6607231\n",
      "Frame: 61, VAR_LAP: 7.102917, SML: 7609127\n",
      "Frame ID of the best frame [Method 1]: 61\n",
      "Frame ID of the best frame [Method 2]: 61\n",
      "Frame: 62, VAR_LAP: 5.444496, SML: 6822894\n",
      "Frame: 63, VAR_LAP: 5.829479, SML: 7041220\n",
      "Frame: 64, VAR_LAP: 5.443960, SML: 6840113\n",
      "Frame: 65, VAR_LAP: 7.215098, SML: 7657058\n",
      "Frame ID of the best frame [Method 1]: 65\n",
      "Frame ID of the best frame [Method 2]: 65\n",
      "Frame: 66, VAR_LAP: 5.693431, SML: 6907139\n",
      "Frame: 67, VAR_LAP: 6.185056, SML: 7093163\n",
      "Frame: 68, VAR_LAP: 5.821382, SML: 6989092\n",
      "Frame: 69, VAR_LAP: 7.977184, SML: 7927993\n",
      "Frame ID of the best frame [Method 1]: 69\n",
      "Frame ID of the best frame [Method 2]: 69\n",
      "Frame: 70, VAR_LAP: 6.173287, SML: 7154538\n",
      "Frame: 71, VAR_LAP: 6.823117, SML: 7465074\n",
      "Frame: 72, VAR_LAP: 6.742280, SML: 7451367\n",
      "Frame: 73, VAR_LAP: 9.558622, SML: 8663816\n",
      "Frame ID of the best frame [Method 1]: 73\n",
      "Frame ID of the best frame [Method 2]: 73\n",
      "Frame: 74, VAR_LAP: 8.388294, SML: 8305970\n",
      "Frame: 75, VAR_LAP: 9.672566, SML: 8772569\n",
      "Frame ID of the best frame [Method 1]: 75\n",
      "Frame ID of the best frame [Method 2]: 75\n",
      "Frame: 76, VAR_LAP: 8.354843, SML: 8158339\n",
      "Frame: 77, VAR_LAP: 8.631736, SML: 8232544\n",
      "Frame: 78, VAR_LAP: 10.282779, SML: 8783560\n",
      "Frame ID of the best frame [Method 1]: 78\n",
      "Frame ID of the best frame [Method 2]: 78\n",
      "Frame: 79, VAR_LAP: 13.193061, SML: 9907882\n",
      "Frame ID of the best frame [Method 1]: 79\n",
      "Frame ID of the best frame [Method 2]: 79\n",
      "Frame: 80, VAR_LAP: 11.588864, SML: 9257251\n",
      "Frame: 81, VAR_LAP: 10.939118, SML: 9107327\n",
      "Frame: 82, VAR_LAP: 19.083643, SML: 11313027\n",
      "Frame ID of the best frame [Method 1]: 82\n",
      "Frame ID of the best frame [Method 2]: 82\n",
      "Frame: 83, VAR_LAP: 20.764218, SML: 11712085\n",
      "Frame ID of the best frame [Method 1]: 83\n",
      "Frame ID of the best frame [Method 2]: 83\n",
      "Frame: 84, VAR_LAP: 16.559540, SML: 10509890\n",
      "Frame: 85, VAR_LAP: 17.386318, SML: 10702666\n",
      "Frame: 86, VAR_LAP: 18.740724, SML: 10878842\n",
      "Frame: 87, VAR_LAP: 23.733414, SML: 12302991\n",
      "Frame ID of the best frame [Method 1]: 87\n",
      "Frame ID of the best frame [Method 2]: 87\n",
      "Frame: 88, VAR_LAP: 36.738106, SML: 13830304\n",
      "Frame ID of the best frame [Method 1]: 88\n",
      "Frame ID of the best frame [Method 2]: 88\n",
      "Frame: 89, VAR_LAP: 40.569467, SML: 13606881\n",
      "Frame ID of the best frame [Method 1]: 89\n",
      "Frame: 90, VAR_LAP: 43.175759, SML: 14064190\n",
      "Frame ID of the best frame [Method 1]: 90\n",
      "Frame ID of the best frame [Method 2]: 90\n",
      "Frame: 91, VAR_LAP: 46.120409, SML: 14298137\n",
      "Frame ID of the best frame [Method 1]: 91\n",
      "Frame ID of the best frame [Method 2]: 91\n",
      "Frame: 92, VAR_LAP: 65.990895, SML: 16736865\n",
      "Frame ID of the best frame [Method 1]: 92\n",
      "Frame ID of the best frame [Method 2]: 92\n",
      "Frame: 93, VAR_LAP: 127.964027, SML: 19992636\n",
      "Frame ID of the best frame [Method 1]: 93\n",
      "Frame ID of the best frame [Method 2]: 93\n",
      "Frame: 94, VAR_LAP: 202.691546, SML: 21256239\n",
      "Frame ID of the best frame [Method 1]: 94\n",
      "Frame ID of the best frame [Method 2]: 94\n",
      "Frame: 95, VAR_LAP: 231.570070, SML: 22527472\n",
      "Frame ID of the best frame [Method 1]: 95\n",
      "Frame ID of the best frame [Method 2]: 95\n",
      "Frame: 96, VAR_LAP: 223.608523, SML: 22015366\n",
      "Frame: 97, VAR_LAP: 208.283197, SML: 22897431\n",
      "Frame ID of the best frame [Method 2]: 97\n",
      "Frame: 98, VAR_LAP: 287.931694, SML: 23116686\n",
      "Frame ID of the best frame [Method 1]: 98\n",
      "Frame ID of the best frame [Method 2]: 98\n",
      "Frame: 99, VAR_LAP: 305.828529, SML: 23613420\n",
      "Frame ID of the best frame [Method 1]: 99\n",
      "Frame ID of the best frame [Method 2]: 99\n",
      "Frame: 100, VAR_LAP: 306.017167, SML: 23395378\n",
      "Frame ID of the best frame [Method 1]: 100\n",
      "Frame: 101, VAR_LAP: 322.410179, SML: 24701402\n",
      "Frame ID of the best frame [Method 1]: 101\n",
      "Frame ID of the best frame [Method 2]: 101\n",
      "Frame: 102, VAR_LAP: 316.325105, SML: 23468944\n",
      "Frame: 103, VAR_LAP: 319.230266, SML: 23953931\n",
      "Frame: 104, VAR_LAP: 113.359925, SML: 17486392\n",
      "Frame: 105, VAR_LAP: 111.814966, SML: 17885582\n",
      "Frame: 106, VAR_LAP: 105.552424, SML: 17295858\n",
      "Frame: 107, VAR_LAP: 136.890131, SML: 19204928\n",
      "Frame: 108, VAR_LAP: 105.134938, SML: 17945502\n",
      "Frame: 109, VAR_LAP: 75.645927, SML: 17320649\n",
      "Frame: 110, VAR_LAP: 54.379255, SML: 16286592\n",
      "Frame: 111, VAR_LAP: 48.895493, SML: 15471594\n",
      "Frame: 112, VAR_LAP: 60.275116, SML: 17134893\n",
      "Frame: 113, VAR_LAP: 61.448494, SML: 18021669\n",
      "Frame: 114, VAR_LAP: 132.008943, SML: 23417120\n",
      "Frame: 115, VAR_LAP: 121.410719, SML: 21090263\n",
      "Frame: 116, VAR_LAP: 140.305763, SML: 22208078\n",
      "Frame: 117, VAR_LAP: 129.213931, SML: 21348463\n",
      "Frame: 118, VAR_LAP: 69.007871, SML: 18915873\n",
      "Frame: 119, VAR_LAP: 14.699742, SML: 10887436\n",
      "Frame: 120, VAR_LAP: 14.805653, SML: 10976023\n",
      "Frame: 121, VAR_LAP: 13.633641, SML: 10577770\n",
      "Frame: 122, VAR_LAP: 17.144795, SML: 11701973\n",
      "Frame: 123, VAR_LAP: 12.815819, SML: 10329007\n",
      "Frame: 124, VAR_LAP: 13.933374, SML: 10793658\n",
      "Frame: 125, VAR_LAP: 18.857412, SML: 12093362\n",
      "Frame: 126, VAR_LAP: 15.362382, SML: 11053696\n",
      "Frame: 127, VAR_LAP: 14.262517, SML: 10758343\n",
      "Frame: 128, VAR_LAP: 14.451004, SML: 10858558\n",
      "Frame: 129, VAR_LAP: 16.589074, SML: 11811205\n",
      "Frame: 130, VAR_LAP: 14.118214, SML: 10595312\n",
      "Frame: 131, VAR_LAP: 14.464243, SML: 10794404\n",
      "Frame: 132, VAR_LAP: 13.094915, SML: 10374505\n",
      "Frame: 133, VAR_LAP: 15.411975, SML: 11458469\n",
      "Frame: 134, VAR_LAP: 13.694669, SML: 10687367\n",
      "Frame: 135, VAR_LAP: 15.878253, SML: 11083228\n",
      "Frame: 136, VAR_LAP: 14.451293, SML: 10827505\n",
      "Frame: 137, VAR_LAP: 15.543936, SML: 11489346\n",
      "Frame: 138, VAR_LAP: 13.117364, SML: 10411048\n",
      "Frame: 139, VAR_LAP: 14.705420, SML: 10784131\n",
      "Frame: 140, VAR_LAP: 22.924445, SML: 12107088\n",
      "Frame: 141, VAR_LAP: 17.302220, SML: 11906602\n",
      "Frame: 142, VAR_LAP: 13.860893, SML: 10668432\n",
      "Frame: 143, VAR_LAP: 13.783315, SML: 10668779\n",
      "Frame: 144, VAR_LAP: 12.980443, SML: 10389508\n",
      "Frame: 145, VAR_LAP: 15.833256, SML: 11521012\n",
      "Frame: 146, VAR_LAP: 13.264424, SML: 10515417\n",
      "Frame: 147, VAR_LAP: 13.680633, SML: 10655075\n",
      "Frame: 148, VAR_LAP: 13.204176, SML: 10465117\n",
      "Frame: 149, VAR_LAP: 16.097573, SML: 11475060\n",
      "Frame: 150, VAR_LAP: 12.205401, SML: 9997391\n",
      "Frame: 151, VAR_LAP: 13.432683, SML: 10498618\n",
      "Frame: 152, VAR_LAP: 12.430559, SML: 10136020\n",
      "Frame: 153, VAR_LAP: 16.385307, SML: 11566046\n",
      "Frame: 154, VAR_LAP: 12.832158, SML: 10319369\n",
      "Frame: 155, VAR_LAP: 12.638750, SML: 10299656\n",
      "Frame: 156, VAR_LAP: 11.892079, SML: 10011288\n",
      "Frame: 157, VAR_LAP: 14.375310, SML: 11100176\n",
      "Frame: 158, VAR_LAP: 13.293710, SML: 10369663\n",
      "Frame: 159, VAR_LAP: 12.557333, SML: 10312736\n",
      "Frame: 160, VAR_LAP: 11.777280, SML: 9989229\n",
      "Frame: 161, VAR_LAP: 14.044458, SML: 11036476\n",
      "Frame: 162, VAR_LAP: 13.482660, SML: 10283777\n",
      "Frame: 163, VAR_LAP: 12.349034, SML: 10222931\n",
      "Frame: 164, VAR_LAP: 12.087718, SML: 10108062\n",
      "Frame: 165, VAR_LAP: 13.787933, SML: 10917821\n",
      "Frame: 166, VAR_LAP: 13.759559, SML: 10409267\n",
      "Frame: 167, VAR_LAP: 12.476608, SML: 10213533\n",
      "Frame: 168, VAR_LAP: 11.836928, SML: 9981746\n",
      "Frame: 169, VAR_LAP: 13.813734, SML: 10965892\n",
      "Frame: 170, VAR_LAP: 14.115618, SML: 10521345\n",
      "Frame: 171, VAR_LAP: 12.728684, SML: 10212665\n",
      "Frame: 172, VAR_LAP: 12.229269, SML: 10048747\n",
      "Frame: 173, VAR_LAP: 14.549796, SML: 11138278\n",
      "Frame: 174, VAR_LAP: 13.389962, SML: 10666645\n",
      "Frame: 175, VAR_LAP: 15.712429, SML: 11098129\n",
      "Frame: 176, VAR_LAP: 12.943588, SML: 10148227\n",
      "Frame: 177, VAR_LAP: 12.411079, SML: 9885521\n",
      "Frame: 178, VAR_LAP: 11.477252, SML: 9606059\n",
      "Frame: 179, VAR_LAP: 15.123368, SML: 11115904\n",
      "Frame: 180, VAR_LAP: 13.249403, SML: 10139273\n",
      "Frame: 181, VAR_LAP: 12.813380, SML: 9909772\n",
      "Frame: 182, VAR_LAP: 22.255887, SML: 12207087\n",
      "Frame: 183, VAR_LAP: 36.234436, SML: 14297221\n",
      "Frame: 184, VAR_LAP: 112.406116, SML: 19912836\n",
      "Frame: 185, VAR_LAP: 146.582249, SML: 21677391\n",
      "================================================\n",
      "Frame ID of the best frame [Method 1]: 101\n",
      "Frame ID of the best frame [Method 2]: 101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.5), np.float64(2559.5), np.float64(719.5), np.float64(-0.5))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "Implementing Variance of absolute values of Laplacian method",
     "locked": true,
     "points": "10",
     "solution": false
    },
    "editable": false,
    "deletable": false,
    "ExecuteTime": {
     "end_time": "2025-01-20T23:04:52.663652Z",
     "start_time": "2025-01-20T23:04:52.660632Z"
    }
   },
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "Implementing Sum Modified Laplacian (SML) method",
     "locked": true,
     "points": "10",
     "solution": false
    },
    "editable": false,
    "deletable": false,
    "ExecuteTime": {
     "end_time": "2025-01-20T23:04:52.676259Z",
     "start_time": "2025-01-20T23:04:52.673175Z"
    }
   },
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "editable": false,
    "deletable": false
   },
   "source": [
    "# <font style = \"color:rgb(50,120,229)\">Additional Exercise</font>\n",
    "\n",
    "In this assignment you implemented 2 of the several measures of focus. Now, try to implement the rest of the measures of focus and see if your output matches the output you obtained using the above 2 methods. \n",
    "\n",
    "Which method do you think is the best one and why? Share your answers on the discussion forum."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
